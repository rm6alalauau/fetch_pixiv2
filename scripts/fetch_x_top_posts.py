import os
import json
import time
import requests
from playwright.sync_api import sync_playwright
import urllib.parse

# --- å¾ GitHub Secrets è®€å–ç§˜å¯† ---
APPS_SCRIPT_URL = os.getenv('APPS_SCRIPT_URL')
X_APPS_SCRIPT_SECRET = os.getenv('X_APPS_SCRIPT_SECRET')
AUTH_JSON_CONTENT = os.getenv('AUTH_JSON_CONTENT')

# --- æŠ“å–é…ç½® ---
SEARCH_KEYWORD = "#ãƒ–ãƒ©ãƒ€ã‚¹2"
TARGET_URL = f"https://x.com/search?q={urllib.parse.quote(SEARCH_KEYWORD)}&src=typed_query&f=top"
MAX_POSTS = 20
SCROLL_ATTEMPTS = 15 # ã€æ–°è¨­å®šã€‘å®šç¾©è¦å‘ä¸‹æ»¾å‹•å¹¾æ¬¡ä¾†è¼‰å…¥æ›´å¤šå…§å®¹

def main():
    if not all([APPS_SCRIPT_URL, X_APPS_SCRIPT_SECRET, AUTH_JSON_CONTENT]):
        print("âŒ Missing required environment variables. Aborting.")
        return

    with open("auth.json", "w") as f:
        f.write(AUTH_JSON_CONTENT)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(storage_state="auth.json")
        page = context.new_page()

        try:
            print(f"Navigating to: {TARGET_URL}")
            page.goto(TARGET_URL, timeout=90000)
            page.wait_for_selector('article[data-testid="tweet"]', timeout=60000)
            print("Initial page loaded. Starting to scroll down to load more content...")

            # ã€æ ¸å¿ƒä¿®æ”¹ 1ã€‘åŠ å…¥æ»¾å‹•è¿´åœˆ
            for i in range(SCROLL_ATTEMPTS):
                print(f"  - Scrolling down... (Attempt {i + 1}/{SCROLL_ATTEMPTS})")
                # åŸ·è¡Œ JS ä¾†æ»¾å‹•åˆ°é é¢åº•éƒ¨
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                # ç­‰å¾… 2-3 ç§’è®“æ–°å…§å®¹è¼‰å…¥
                time.sleep(3)
            
            print("\nFinished scrolling. Now parsing all found tweets...")

            tweet_elements = page.locator('article[data-testid="tweet"]').all()
            print(f"Found {len(tweet_elements)} potential tweets on the page after scrolling.")

            all_posts = []
            seen_tweet_links = set() # ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘ç”¨æ–¼é˜²æ­¢é‡è¤‡

            for i, tweet in enumerate(tweet_elements):
                if len(all_posts) >= MAX_POSTS:
                    print(f"Reached target of {MAX_POSTS} posts. Stopping parse.")
                    break
                
                post_data = {}
                try:
                    # é¦–å…ˆå˜—è©¦ç²å–ç¨ä¸€ç„¡äºŒçš„é€£çµä¾†åˆ¤æ–·æ˜¯å¦é‡è¤‡
                    post_link = ""
                    link_element = tweet.locator('a[href*="/status/"]').first
                    if link_element.count() > 0:
                        href = link_element.get_attribute('href')
                        if href:
                             post_link = f"https://x.com{href}"
                    
                    if not post_link or post_link in seen_tweet_links:
                        continue # å¦‚æœæ²’æœ‰é€£çµæˆ–å·²è™•ç†éï¼Œå‰‡è·³é

                    # è§£æå…¶ä»–å…§å®¹
                    post_data["Link"] = post_link
                    post_data["Title"] = tweet.locator('div[data-testid="tweetText"]').inner_text(timeout=5000)
                    post_data["Author"] = tweet.locator('div[data-testid="User-Name"] a span').first.inner_text(timeout=5000)
                    
                    handle_element = tweet.locator('div[data-testid="User-Name"] a > div > span').first
                    author_handle = handle_element.inner_text(timeout=5000) if handle_element.count() > 0 else ""
                    post_data["AuthorProfile"] = f"https://x.com/{author_handle.replace('@', '')}"
                    
                    # ã€æ ¸å¿ƒä¿®æ”¹ 3ã€‘æ›´ç©©å¥çš„åœ–ç‰‡è™•ç†
                    image_url = ""
                    image_locator = tweet.locator('div[data-testid="tweetPhoto"] img').first
                    if image_locator.count() > 0: # å…ˆåˆ¤æ–·å…ƒç´ æ˜¯å¦å­˜åœ¨
                        image_url = image_locator.get_attribute('src', timeout=5000)
                    post_data["Image"] = image_url
                    post_data["Thumbnail"] = image_url

                    hashtags_elements = tweet.locator('a[href*="/hashtag/"]').all()
                    post_data["Hashtag"] = " ".join([h.inner_text(timeout=2000) for h in hashtags_elements])
                    post_data["Time"] = ""

                    all_posts.append(post_data)
                    seen_tweet_links.add(post_link) # æ¨™è¨˜ç‚ºå·²è™•ç†
                    print(f"  - Parsed tweet #{len(all_posts)}: {post_data['Title'][:30]}...")

                except Exception as e:
                    print(f"  - WARN: Skipping a tweet at index {i} due to parsing error: {e}")
            
            print(f"\nâœ… Successfully parsed {len(all_posts)} unique top tweets.")

            if all_posts:
                # å¾ŒçºŒé‚è¼¯ä¸è®Š
                payload = {"secret": X_APPS_SCRIPT_SECRET, "data": all_posts}
                headers = {"Content-Type": "application/json"}
                print(f"Posting {len(all_posts)} posts to Apps Script...")
                response = requests.post(APPS_SCRIPT_URL, data=json.dumps(payload, ensure_ascii=False).encode('utf-8'), headers=headers)
                response.raise_for_status()
                print(f"âœ… Successfully posted data. Response: {response.text}")

        except Exception as e:
            print(f"âŒ An critical error occurred during scraping: {e}")
            page.screenshot(path="error_screenshot.png")
            print("ğŸ“¸ An error screenshot has been saved.")
            raise e
        finally:
            browser.close()

if __name__ == "__main__":
    main()
