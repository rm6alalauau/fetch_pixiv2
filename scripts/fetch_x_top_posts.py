import os
import json
import time
import requests
from playwright.sync_api import sync_playwright
import urllib.parse

# --- å¾ GitHub Secrets è®€å–ç§˜å¯† ---
APPS_SCRIPT_URL = os.getenv('APPS_SCRIPT_URL')
X_APPS_SCRIPT_SECRET = os.getenv('X_APPS_SCRIPT_SECRET')
AUTH_JSON_CONTENT = os.getenv('AUTH_JSON_CONTENT')

# --- æŠ“å–é…ç½® ---
SEARCH_KEYWORD = "#ãƒ–ãƒ©ãƒ€ã‚¹2"
# ã€ä¿®æ”¹é» 1ã€‘ç¢ºä¿é—œéµå­—è¢«æ­£ç¢ºç·¨ç¢¼åˆ° URL ä¸­
TARGET_URL = f"https://x.com/search?q={urllib.parse.quote(SEARCH_KEYWORD)}&src=typed_query&f=top"
MAX_POSTS = 20

def main():
    if not all([APPS_SCRIPT_URL, X_APPS_SCRIPT_SECRET, AUTH_JSON_CONTENT]):
        print("âŒ Missing required environment variables. Aborting.")
        return

    # å°‡å¾ Secret è®€å–çš„ JSON å­—ä¸²å¯«å…¥æš«å­˜æª”æ¡ˆ
    with open("auth.json", "w") as f:
        f.write(AUTH_JSON_CONTENT)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(storage_state="auth.json")
        page = context.new_page()

        try:
            print(f"Navigating to: {TARGET_URL}")
            # ã€ä¿®æ”¹é» 2ã€‘æ”¾å¯¬ goto çš„ç­‰å¾…æ¢ä»¶ï¼Œä½†å¢åŠ ç¸½è¶…æ™‚æ™‚é–“åˆ° 90 ç§’
            page.goto(TARGET_URL, timeout=90000)

            # ã€ä¿®æ”¹é» 3ã€‘é€™æ˜¯æœ€é—œéµçš„ä¿®æ”¹ï¼æ˜ç¢ºç­‰å¾…æ¨æ–‡å®¹å™¨å‡ºç¾ï¼Œè€Œä¸æ˜¯ç­‰ç¶²è·¯éœæ­¢
            print("Waiting for tweet container to appear...")
            page.wait_for_selector('article[data-testid="tweet"]', timeout=60000)
            print("Tweet container found. Parsing tweets...")
            
            # çµ¦é é¢ä¸€é»é¡å¤–æ™‚é–“æ¸²æŸ“å‹•æ…‹å…§å®¹
            time.sleep(3)

            tweet_elements = page.locator('article[data-testid="tweet"]').all()
            print(f"Found {len(tweet_elements)} potential tweets on the page.")

            all_posts = []
            for i, tweet in enumerate(tweet_elements[:MAX_POSTS]):
                post_data = {}
                try:
                    # ã€ä¿®æ”¹é» 4ã€‘å°æ¯å€‹æ¬„ä½çš„æŠ“å–é€²è¡Œæ›´ç´°ç·»çš„éŒ¯èª¤è™•ç†
                    post_data["Title"] = tweet.locator('div[data-testid="tweetText"]').inner_text(timeout=5000)
                    
                    author_name_element = tweet.locator('div[data-testid="User-Name"] a span').first
                    post_data["Author"] = author_name_element.inner_text(timeout=5000)
                    
                    author_handle = ""
                    handle_element = tweet.locator('div[data-testid="User-Name"] a > div > span').first
                    if handle_element:
                        author_handle = handle_element.inner_text(timeout=5000)
                    post_data["AuthorProfile"] = f"https://x.com/{author_handle.replace('@', '')}" if author_handle else ""
                    
                    post_link = ""
                    link_element = tweet.locator('a[href*="/status/"]').first
                    if link_element:
                        href = link_element.get_attribute('href')
                        if href:
                             post_link = f"https://x.com{href}"
                    post_data["Link"] = post_link

                    image_url = ""
                    image_element = tweet.locator('div[data-testid="tweetPhoto"] img').first
                    if image_element:
                        image_url = image_element.get_attribute('src', timeout=5000)
                    post_data["Image"] = image_url
                    post_data["Thumbnail"] = image_url

                    hashtags_elements = tweet.locator('a[href*="/hashtag/"]').all()
                    post_data["Hashtag"] = " ".join([h.inner_text(timeout=2000) for h in hashtags_elements])
                    
                    post_data["Time"] = "" # æ™‚é–“ä¾ç„¶è¼ƒé›£æŠ“å–ï¼Œæš«æ™‚ç•™ç©º

                    all_posts.append(post_data)
                    print(f"  - Parsed tweet #{i+1}: {post_data['Title'][:30]}...")

                except Exception as e:
                    print(f"  - WARN: Skipping a tweet at index {i} due to parsing error: {e}")
            
            print(f"\nâœ… Successfully parsed {len(all_posts)} top tweets.")

            if all_posts:
                payload = {"secret": X_APPS_SCRIPT_SECRET, "data": all_posts}
                headers = {"Content-Type": "application/json"}
                print(f"Posting {len(all_posts)} posts to Apps Script...")
                response = requests.post(APPS_SCRIPT_URL, data=json.dumps(payload, ensure_ascii=False).encode('utf-8'), headers=headers)
                response.raise_for_status()
                print(f"âœ… Successfully posted data. Response: {response.text}")

        except Exception as e:
            print(f"âŒ An critical error occurred during scraping: {e}")
            page.screenshot(path="error_screenshot.png")
            print("ğŸ“¸ An error screenshot has been saved.")
            raise e
        finally:
            browser.close()

if __name__ == "__main__":
    main()
